---
title: "Hidden Markov Models"
author: "Jaeyong Lee"
output:
  html_notebook:
    toc: yes
    code_folding: "none"
---

<style type="text/css">
h1.title {
  font-size: 30px;
  text-align: center;
}
h3.subtitle {
  font-size: 20px;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
    font-size: 18px;
  text-align: right;
}
body{
   font-size: 17px;  # body is for normal text
}
td{
   font-size: 12px;  # td is for table data
}
</style>

\
\

Many of the texts and figures are from 'Speech and Language Processing' by Daniel Jurafsky & James H. Martin.

\
\
\
\

### 1. Markov Chains

\

A **_Markov chain_** is a model that tells us something about the probabilities of sequences of random variables, states, each of which can take on values from some set. A Markov chain makes a strong assumption that if we want to predict the future in the sequence, all that matters is the current state. It is based on the following assumption.

\

(First-order) **_Markov assumption:_**

\

$$
P(q_i=a|q_1,\dotsc,q_{i-1}) = P(q_i=a|q_{i-1}) \tag{A.1}
$$

\
\

The following figure shows an example of a Markov chain.

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss1.png)

\

Figure A.1 shows a Markov chain for assigning a probability to a sequence of states. The states are represented as nodes in the graph, and the transitions with their probabilities, as edges. Since the transitions are probabilities, **the values of arcs leaving a given state must sum to 1**.

\
\

Formally, a Markov chain is specified by the following components:

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss2.png)

\

The transitional probability matrix (TPM) for Fig A.1 (a) would be as following

\

$$
\mathrm{A} = 
\begin{bmatrix}
0.6 & 0.1 & 0.3 \\ 
0.1 & 0.8 & 0.1 \\ 
0.3 & 0.1 & 0.6
\end{bmatrix}
$$

\
\
\

#### Example:

\

Let's use the sample probabilities in Fig A.1 (a) (with $\pi = [.1, \ .7, \ .2]$) to compute the probability of each of the following sequences:

\

$$
\mathrm{hot \ \ hot \ \ hot \ \ hot} \tag{1}
$$

$$
\mathrm{cold \ \ hot \ \ cold \ \ hot} \tag{2}
$$

\

```{r}
# transitional probability matrix (TPM):
tpm <- matrix(c(0.6,0.1,0.3,0.1,0.8,0.1,0.3,0.1,0.6),nrow=3,byrow=T)
tpm
```

```{r}
# Initial probability distribution
pi <- matrix(c(0.1, 0.7, 0.2), nrow=1, byrow=T)  # this was given
pi
```

\

For sequence (1), the probability is

\

$$
\pi_1 \times a_{11} \times a_{11} \times a_{11}
$$

\

where $a_{ij}$ is the (i,j) component of transitional probability matrix A.

\

```{r}
pi[,1]*tpm[1,1]*tpm[1,1]*tpm[1,1]
```


\
\

For sequence (2), the probability is

\

$$
\pi_2 \times a_{21} \times a_{12} \times a_{21}
$$

\

```{r}
options(scipen = 1)  # to remove scientific notation
pi[,2]*tpm[2,1]*tpm[1,2]*tpm[2,1]
```

\
\
\
\

### 2. The Hidden Markov Model

\

A Markov chain is useful when we need to compute a probability for a sequence of observable events. In many cases, however, the events we are interested in are hidden: we don't observe them directly.

\

**_Hidden Markov model (HMM)_**:

A **hidden Markov model** allows us to talk about **both observed** events and **hidden** events that we think of as causal factors in our probabilistic model. An HMM is specified by the following components:

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss3.png)

\
\

A first-order HMM instantiates two simplifying assumptions. 

\

First, as with a first-order Markov chain, the probability of a particular state depends only on the previous state.

\

- **_Markov assumption_**:

\

$$
P(q_i|q_1,\dotsc,q_{i-1}) = P(q_i|q_{i-1}) \tag{A.4}
$$

\

Second, the probability of an output observation $o_i$ depends only on the state that produced the observation $q_i$ and not on any other states or any other observations.

\

- **_Output independence_**:

\

$$
P(o_i|q_1,\dotsc,q_{T},o_1,\dotsc,o_{T}) = P(o_i|q_i) \tag{A.5}
$$

\
\
\

#### Example:

\

Consider such a task: given a sequence of observations $O$ ({1,2,3} representing the number of ice creames eaten by Jason on a given day), find the "hidden" sequence of $Q$ of weather states (H or C) which caused Jason to eat the ice cream. This is described in the following figure.

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss4.png)

\

The initial probability matrix (vector), $\pi$, is

$$
\pi = \begin{bmatrix}
0.2 & 0.8
\end{bmatrix}
$$

\

The transition probability matrix, $A$, for this example is

\

$$
A = \begin{bmatrix}
0.5 & 0.5 \\ 
0.4 & 0.6
\end{bmatrix}
$$

\

The observation likelihoods (emission probability) matrix, $B$, for this example is

$$
B = \begin{bmatrix}
0.5 & 0.4 & 0.1 \\ 
0.2 & 0.4 & 0.4
\end{bmatrix}
$$

\

Note that $B$ is of **conditional** probabilities.

\

Also note that, all the matrices above must have each **ROW SUM TO 1** because they are probabilities.

\
\
\

Note that, Hidden Markov models should be characterized by **_three fundamental problems_**:

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss5.png)

\

where, $A$: transition probability matrix (TPM) and $B$: a sequence of observation likelihoods (also called, emission probabilities)

\
\
\
\

### 3. Likelihood Computation: The Forward Algorithm

\

The first problem is to compute the likelihood of a particular observation sequence. For example, given the ice cream example in Fig. A.2, what is the probability of the sequence $3 \ \ 1 \ \ 3$? This is formally described as the following.

\

**_Computing Likelihood_**:

Given an HMM $\lambda = (A,B)$ and an observation sequence $O$, determine the likelihood $P(O|\lambda)$.

\
\

**If we KNOW the sequence hidden states**, we can calculate the likelihood as the following.

\

Recall that for HMM, **each hidden state produces only a single observation**. Thus, the sequence of hidden states and the sequence of observations have the same length (note that segmental HMMs and semi-HMMs don't satisfy this).  Given this **one-to-one mapping** and the Markov assumptions for a particular hidden state sequence $Q = q_0, q_1, \dotsc, q_T$ and an observation sequence $O = o_1, o_2, \dotsc, o_T$, the likelihood of the observation sequence is

\

$$
P(O|Q) = \prod_{i=1}^{T}P(o_i|q_i) \tag{A.6}
$$

\
\

#### Example:

\

The computation of the forward probability for the ice cream observation $3 \ \ 1 \ \ 3$ from a hidden state sequence $hot \ \ hot \ \ cold$ is as the following.

\

$$
P(3 \ \ 1 \ \ 3|hot \ \ hot \ \ cold) = P(3|hot) \times P(1|hot) \times P(3|cold) \tag{A.7}
$$

\

The following figure shows a graphic representation of this computation.

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss6.png)

\
\
\

Most of the time, however, we don't know the sequence of hidden states.

\

**If we DON'T KNOW the sequence of hidden states**, we can calculate the likelihood as the following.

\

First, **compute the joint probability** of being in a particular hidden state sequence $Q$ and generating a particular observation sequence $O$:

\

$$
P(O,Q) = P(O|Q) \times P(Q) = \prod_{i=1}^{T} P(o_i|q_i) \times \prod_{i=1}^{T} P(q_i|q_{i-1}) \tag{A.8}
$$

\

Then we can **compute the total probability of the observation sequence** just by summing over all possible hidden state sequences.

\

$$
P(O) = \sum_{Q} P(O,Q) = \sum_{Q} P(O|Q)P(Q) \tag{A.10}
$$

\
\

#### Example:

\

For the ice cream example, the computation of the joint probability of the observation $3 \ \ 1 \ \ 3$ and the hidden state sequence $hot \ \ hot \ \ cold$ is

\

$$
\begin{align*}
P(3 \ \ 1 \ \ 3, \ hot \ \ hot \ \ cold) = \ & P(3|hot) \times P(1|hot) \times P(3|cold) \\ 
 &\times P(hot|start) \times P(hot|hot) \times  P(cold|hot)
\end{align*}
$$

\

and the total probability of the observations is

$$
P(3 \ \ 1 \ \ 3) = P(3 \ \ 1 \ \ 3, \ cold \ \ cold \ \ cold) + P(3 \ \ 1 \ \ 3, \ cold \ \ cold \ \ hot) + P(3 \ \ 1 \ \ 3, \ hot \ \ hot \ \ cold) + \dots
$$

\
\
\

For an HMM with $N$ hidden states and an observation sequence of $T$ observations, there are $N^T$ possible hidden sequences. For real tasks, where $N$ and $T$ are both large, $N^T$ can be too much to compute the total observation likelihood.

\
\

**_Forward algorithm_**:

Instead of using such an extremely exponential algorithm, we use an efficient $O(N^2T)$ algorithm called the **forward algorithm**. The forward algorithm is a kind of **dynamic programming** algorithm, that is, an algorithm that uses a table to store intermediate values as it builds up the probability of the observation sequence. The forward algorithm computes the observation probability by summing over the probabilities of all possible hidden state paths that could generate the observation sequence, but it does so efficiently by implicitly folding each of these paths into a single **_forward trellis_**.

\

The following figure shows an example of the forward trellis.

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss7.png)

\

Each cell of the forward algorithm trellis **$\alpha_t(j)$ represents the probability of being in state $j$ after seeing the first $t$ observations, given $\lambda = (A,B)$** (where, $A$: transition probability matrix (TPM) and $B$: a sequence of observation likelihoods).

The value of each cell $\alpha_t(j)$ is computed by summing over the probabilities of every path that could lead us to this cell. Formally, each cell expresses the following probability:

\

$$
\alpha_t(j) = P(o_1, o_2, \dots, o_t, q_t=j|\lambda) \tag{A.11}
$$

\

where, $q_t = j$ means "the $t$th state in the sequence of states is state $j$". 

\

We compute this probability $\alpha_t(j)$ by summing over the extensions of all the paths that lead to the current cell. For a given state $q_j$ at time $t$, the value $\alpha_t(j)$ is computed as

\

$$
\alpha_t(j) = \sum_{i=1}^{N} \alpha_{t-1}(i)a_{ij}b_j(o_t) \tag{A.12}
$$

\

The three factors that are multiplied in Eq. A.12 in extending the previous paths to compute the forward probability at time $t$ are

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss8.png)

\
\

#### Example:

\

Consider the computation in Fig. A.5 of $\alpha_2(2)$, the forward probability of being at time step 2 in state 2 having generate the partial observation $3 \ \ 1$.

We compute by extending the $\alpha$ probabilities from time step 1, via two paths, each extension consisting of the three factors above: 

\

$$
\alpha_1(1) \times P(H|C) \times P(1|H)
$$

\

where, $\alpha_1(1)$: the previous forward path probability, $P(H|C)$: the transition probability, and $P(1|H)$: the state observation likelihood, and

\

$$
\alpha_1(2) \times P(H|H) \times P(1|H)
$$

\

where, $\alpha_1(2)$: the previous forward path probability, $P(H|H)$: the transition probability, and $P(1|H)$: the state observation likelihood.

\
\

The following figure shows another visualization of this induction step for computing the value in one new cell of the trellis.

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss9.png)

\
\
\

The following is the pseudocode for the forward algorithm.

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss10.png)

\
\

The following is the statement of the definitional recursion of the forward algorithm.

\

1. Initialization:

\

$$
\alpha_1(j) = \pi_jb_j(o_1) \ ,\ \ 1 \leq j \leq N
$$

\

2. Recursion:

\

$$
\alpha_t(j) = \sum_{i=1}^{N} \alpha_{t-1}(i)a_{ij}b_j(o_t) \ , \ \ 1 \leq j \leq N, \ 1 \leq t \leq T
$$

\

3. Termination:

\

$$
P(O|\lambda) = \sum_{i=1}^{N} \alpha_T(i)
$$

\
\
\

#### Implementing the forward algorithm using the 'HMM' package:

\

Refer to the ice cream example,

```{r}
library(HMM)
# Initialise HMM
hmm = initHMM(
  States=c("C","H"),
  Symbols=c(1,2,3),
  startProbs=c(.2,.8),
  transProbs=matrix(c(.5,.5,.4,.6),nrow=2,byrow=T),  # (num. of states)X(num. of states) sized matrix
  emissionProbs=matrix(c(.5,.4,.1,.2,.4,.4),nrow=2,byrow=T)  # (num. of states)X(num. of obs.) sized matrix
)
print(hmm)
```

```{r}
# Sequence of observations
observations = c(3,1)  # o_1, o_2

# Calculate forward probabilities
logForwardProbabilities = forward(hmm,observations)
exp(logForwardProbabilities)
```

where, "states C,H" are the states of $j$ and "index 1,2" are the time step $t$ of $\alpha_t(j)$.

\

This is the same as the result of Fig. A.5.

\
\
\
\

### 4. Decoding: The Viterbi Algorithm

\

**_Decoding_**:

Given as input an HMM $\lambda = (A,B)$ and a sequence of observations $O = o_1, o_2, \dots, o_T$, **find the most probable sequence of states** $Q = q_1q_2\cdots q_T$.

\

We might propose to find the best sequence as follows: For each possible hidden state sequence, we could run the forward algorithm and compute the likelihood of the observation sequence given that hidden state sequence. Then we could **choose the hidden state sequence with the maximum observation likelihood**. However, this is computationally very expensive.

\

**_Viterbi algorithm_**:

The most common decoding algorithms for HMMs is the **Viterbi algorithm**. Like the forward algorithm, it is a kind of **dynamic programming** that make uses of a dynamic programming trellis (and it strongly resembles the minimum edit distance algorithm).

\

The following figure shows an example of the Viterbi trellis for computing the best hidden state sequence for the ice cream example.

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss11.png)

\

The idea is to process the observation sequence left to right, filling out the trellis. Each cell of the trellis, **$v_t(j)$, represents the probability that the HMM is in state $j$ after seeing the first $t$ observations and passing through the most probable state sequence $q_1,...q_{t-1}$, given $\lambda = (A,B)$** (where, $A$: transition probability matrix (TPM) and $B$: a sequence of observation likelihoods).

\

Formally, each cell expresses the probability

\

$$
v_t(j) = \underset{q_1,\dots,q_{t-1}}{\mathrm{max}} P(q_1\cdots q_{t-1},o_1,o_2,\dots,o_t,q_t=j|\lambda) \tag{A.13}
$$

\

Note that we represent the most probable path by taking the maximum over all possible previous state sequences $\underset{q_1,\dots,q_{t-1}}{\mathrm{max}}$. Like other dynamic programming algorithms, Viterbi fills each cell recursively. Given that we had already computed the probability of being in every state at time $t-1$, we compute the Viterbi probability by taking the most probable of the extensions of the paths that lead to the current cell. 

\

For a given state $q_j$ at time $t$, the value $v_t(j)$ is computed as

\

$$
v_t(j) = \underset{i=1}{ \overset{N}{\mathrm{max}}} \ v_{t-1}(i)a_{ij}b_j(o_t) \tag{A.14}
$$

\

The three factors that are multplied in Eq. A.14 for extending the previous paths to compute the Viterbi probability at time $t$ are

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss12.png)

\
\

The following figure shows pseudocode for the Viterbi algorithm.

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss13.png)

\

Note that Viterbi algorithm is identical to the forward algorithm except that it takes the next **max** over the previous path probabilities **instead of the sum**. 

\
\

**_Viterbi backtrace_**:

Also note that the Viterbi algorithm has one component that the forward algorithm doesn't have: **_backpointers_**. The reason is that while the forward algorithm needs to produce an observation likelihood, the Viterbi algorithm must produce a probability and also the most likely state sequence. We compute this best state sequence by keeping track of the path of hidden state that led to each state and then at the end backtracking the best path to the beginning (the Viterbi **backtrace**).

The following figure shows an example of the Viterbi backtrace.

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss14.png)

\
\

The formal definition of the Viterbi recursion is as follows.

\

1. Intialization:

\

$$
\begin{align*}
v_1(j) &= \pi_jb_j(o_1) \ , & \ \ 1 \leq j \leq N \\
b_{t_{1}}(j) &= 0 \ , & \ \ 1\leq j \leq N 
\end{align*}
$$

\

2. Recursion:

\

$$
\begin{align*}
v_t(j) &= \underset{i=1}{\overset{N}{\mathrm{max}}} \ v_{t-1}(i)a_{ij}b_j(o_t) \ , & \ \ 1 \leq j \leq N, 1 \leq t \leq T \\
b_{t_{t}}(j) &= \underset{i=1}{\overset{N}{\mathrm{argmax}}} \ v_{t-1}(i)a_{ij}b_j(o_t) \ , & \ \ 1\leq j \leq N, 1 \leq t \leq T
\end{align*}
$$

\

3. Termination:

\

$$
\begin{align*}
\mathrm{The \ best \ score:} & \ \ & P* &= \underset{i=1}{\overset{N}{\mathrm{max}}} \ v_T(i) \\
\mathrm{The \ start \ of \ backtrace:} & \ \ & q_T* &=  \underset{i=1}{\overset{N}{\mathrm{argmax}}} \ v_T(i)
\end{align*}
$$

\
\
\

#### Implementing the Viterbi algorithm using the 'HMM' package:

\

Refer to the ice cream example,

```{r}
library(HMM)
# Initialise HMM
hmm = initHMM(
  States=c("C","H"),
  Symbols=c(1,2,3),
  startProbs=c(.2,.8),
  transProbs=matrix(c(.5,.5,.4,.6),nrow=2,byrow=T),
  emissionProbs=matrix(c(.5,.4,.2,.1,.4,.4),nrow=3,byrow=T)  # for some reason, the matrix order gets weird here
)
print(hmm)
```

```{r}
# Sequence of observations
observations = c(3,1)  # o_1, o_2

# Compute the most probable path of states for a sequence of observations for a given Hidden Markov Model
viterbi = viterbi(hmm, observations)
print(viterbi)
```

For a given sequence of observations $3 \ \ 1$, the most probable path of states is $hot \ \ cold$.

\

This is the same as the result of Fig. A.10 where $v_2(1) = 0.064$ is the best score.

\
\
\
\

### 5. HMM Training: The Forward-Backward Algorithm

\

The third and the last problem for HMMs is learning the parameters of an HMM, that is, $\lambda = (A,B)$ (where, $A$: transition probability matrix (TPM) and $B$: a sequence of observation likelihoods).

\

**_Learning_**:

Given an observation sequence $O$ and the set of possible states in the HMM, learn the HMM parameters $A$ and $B$ (the transition probability matrix and the emission probability matrix).

\

The input to such a learning algorithm would be an unlabeled sequence of observations $O$ and a vocabulary of potential hidden states $Q$.

\
\
\

#### Example:

\

Let us begin by considering the much simpler case of training a fully visible Markov model, where we know both the temperature and the ice cream count for every day.

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss15.png)

\

This would easily allow us to compute the HMM parameters just by maximum likelihood estimation from the training data. 

\

First, we can compute $\pi$ from the count of the 3 initial hidden states, that is, the three sequences have the starting state as $cold$ twice and $hot$ once:

\

$$
\pi_{cold} = \frac{2}{3}, \ \ \pi_{hot} = \frac{1}{3} \\
$$

$$
\pi = \begin{bmatrix}
\frac{1}{3} & \frac{2}{3}
\end{bmatrix}
$$

\

Next we can directly compute the $A$ matrix from the transitions, ignoring the final hidden states:

\

$$
\begin{align*}
P(cold|cold) = \frac{2}{3}, \ \ P(hot|cold) = \frac{1}{3} \\
P(cold|hot) = \frac{1}{3}, \ \ P(hot|hot) = \frac{2}{3}
\end{align*}
$$

$$
A = \begin{bmatrix}
\frac{2}{3} & \frac{1}{3} \\ 
\frac{1}{3} & \frac{2}{3}
\end{bmatrix}
$$

\

and the $B$ matrix:

\

$$
P(1|cold) = \frac{3}{5} = 0.6, \ \ P(2|cold) = \frac{2}{5} = 0.4, \ \ P(3|cold) = \frac{0}{5} = 0 \\
P(1|hot) = \frac{0}{4} = 0, \ \ P(2|hot) = \frac{1}{4} = 0.25, \ \ P(3|hot) = \frac{3}{4} = 0.75
$$

$$
B = \begin{bmatrix}
0.6 & 0.4 & 0 \\ 
0 & 0.25 & 0.75 
\end{bmatrix}
$$

\
\
\

For a real HMM, however, we cannot compute these counts directly from an observation sequence since we don't know which path of states was taken through the machine for a given input. Moreover, we don't know the counts of being in **any** of the hidden states.

\

**_Baum-Welch algorithm_**:

The standard algorithm for HMM training is the **Baum-Welch (forward-backward) algorithm**, which is a special case of the **Expectation-Maximization (EM) algorithm**. EM is an iterative algorithm, computing an initial estimate for the probabilities, then using those estimates to computing a better estimate, and so on, iteratively improving the probabilities that it learns.

\

Thus the **Baum-Welch** algorithm solves this by **iteratively estimating the counts**. We will start with an estimate for the transition and observation probabilities and then use these estimated probabilities to derive better and better probabilities. And we're going to do this by computing the forward probabilities for an observation and then dividing that probability mass among all the different paths that contributed to this forward probability.

\
\

**_Backward probability_**:

To understand the algorithm, it is crucial to define a useful probability related to the forward probability, called the **backward probability**. The backward probability **$\beta_t(i)$ is the probability of seeing the observations from time $t+1$ to the end, given that we are in state $i$ at time $t$ and given $\lambda = (A,B)$** (where, $A$: transition probability matrix (TPM) and $B$: a sequence of observation likelihoods).

\

$$
\beta_t(i) = P(o_{t+1},o_{t+2},\dots,o_T|q_t=i,\lambda) \tag{A.15}
$$

\

It is computed recursively in a similar manner to the forward algorithm.

\

1. Initialization:

\

$$
\beta_T(i) = 1, \ \ 1 \leq i \leq N
$$

\

2. Recursion:

\

$$
\beta_t(i) = \sum_{j=1}^{N} a_{ij}b_j(o_{t+1})\beta_{t+1}(j), \ \ 1 \leq i \leq N, 1 \leq t \leq T
$$

\

3. Termination:

\

$$
P(O|\lambda) = \sum_{j=1}^{N} \pi_jb_j(o_1)\beta_1(j)
$$

\
\

The following figure illustrates the backward induction step.

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss16.png)

\

We are now ready to see how the forward and backward probabilities can help compute the transition probability $a_{ij}$ and emission probability $b_i(o_t)$ from an observation sequence, **even though the actual path taken through the model is hidden**.

\

Let's begin by seeing how to estimate $\hat{a}_{ij}$ by a variant of simple maximum likelihood estimation:

\

$$
\hat{a}_{ij} = \frac{\mathrm{expected \ number \ of \ transitions \ from \ state} \ i \ \mathrm{to \ state} \ j}{\mathrm{expected \ number \ of \ transitions \ from \ state} \ i} \tag{A.16}
$$

\

The intuition behind computing the numerator is as follows. Assume we had some estimate of the probability that a given transition $i \to j$ was taken at a particular point in time $t$ in the observation sequence. If we knew this probability for each particular time $t$, we could sum over all times $t$ to estimate the total count for the transition $i \to j$.

\

More formally, let's define the probability $\xi_t$ as the probability of being in state $i$ at time $t$ and state $j$ at time $t+1$, given the observation sequence and the model:

\

$$
\xi_t(i,j) = P(q_t=i, q_{t+1}=j|O,\lambda) \tag{A.17}
$$

\

To compute $\xi_t$, we first compute a probability which is similar to $\xi_t$, but differs in including the probability of the observation;

\

$$
\mathrm{not-quite-}\xi_t(i,j) = P(q_t=i,q_{t+1}=j,O|\lambda) \tag{A.18}
$$

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss17.png)

\

Fig. A.12 shows the various probabilities that go into computing $\mathrm{not-quite-}\xi_t$: the transition probability for the arc in question, the $\alpha$ probability before the arc, the $\beta$ probability after the arc, and the observation probability for the symbol just after the arc. These four are multiplied together to produce $\mathrm{not-quite-}\xi_t$ as follows:

\

$$
\mathrm{not-quite-}\xi_t(i,j) = \alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j) \tag{A.19}
$$

\

To compute $\xi_t$ from $\mathrm{not-quite-}\xi_t$, we follow the laws of probability and divide by $P(O|\lambda)$, since

\

$$
P(X|Y,Z) = \frac{P(X,Y|Z)}{P(Y|Z)} \tag{A.20}
$$

\

The probability of the observation given the model is simply the forward probability of the whole utterance (or alternatively, the backward probability of the whole utterance):

\

$$
P(O|\lambda) = \sum_{j=1}^{N} \alpha_t(j)\beta_t(j) \tag{A.21}
$$

\

So, the final equation for $\xi_t$ is

\

$$
\xi_t(i,j) = \frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum_{j=1}^{N}\alpha_t(j)\beta_t(j)} \tag{A.22}
$$

\

The expected number of transitions from time $i$ to state $j$ is then sum over all $t$ of $\xi$. For the estimate of $\hat{a}_{ij}$ in Eq. A.16, we just need one more thing: the total expected number of transitions from state $i$. We can get this by summing over all transitions out of state $i$. Here's the final formula for $\hat{\alpha}_{ij}$:

\

$$
\hat{a}_{ij} = \frac{\sum_{t=1}^{T-1} \xi_t(i,j)}{\sum_{t=1}^{T-1}\sum_{k=1}^{N}\xi_t(i,k)} \tag{A.23}
$$

\

We also need a formula for recomputing the observation probability. This is the probability of a given symbol $v_k$ from the observation vocabulary $V$, given a state $j$: $\hat{\beta}_j(v_k)$. We will do this by trying to compute

\

$$
\hat{b}_j(v_k) = \frac{\mathrm{expected \ number \ of \ times \ in \ state} \ j \ \mathrm{and \ observing \ symbol} \ v_k}{\mathrm{expected \ number \ of \ times \ in \ state} \ j} \tag{A.24}
$$

\

For this, we will need to know the probability of being in state $j$ at time $t$, which we will call $\gamma_t(j)$:

\

$$
\gamma_t(j) = P(q_t=j|O,\lambda) \tag{A.25}
$$

\

Once again, we will compute this by including the observation sequence in the probability:

\

$$
\gamma_t(j) = \frac{P(q_t=j,O|\lambda)}{P(O|\lambda)} \tag{A.26}
$$

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss18.png)

\

Fig. A.13 shows that the numerator of Eq. A.26 is just the product of the forward probability and the backward probability:

\

$$
\gamma_t(j) = \frac{\alpha_t(j)\beta_t(j)}{P(O|\lambda)} \tag{A.27}
$$

\

Now we are ready to compute $b$. For the numerator, we sum $\gamma_t(j)$ for all time steps $t$ in which the observation $o_t$ is the symbol $v_k$ that we are interested in. For the denominator, we sum $\gamma_t(j)$ over all time steps $t$. The result is the percentage of the times that we were in state $j$ and saw symbol $v_k$:

\

$$
\hat{b}_j(v_k) = \frac{\sum_{t=1 \ s.t. \ O_t=v_k}^{T} \gamma_t(j)}{\sum_{t=1}^{T} \gamma_t(j)} \tag{A.28}
$$

\

where, $\sum_{t=1 \ s.t. \ O_t=v_k}^{T}$ means "sum over all $t$ for which the observation at time $t$ was $v_k$".

\
\

We now have ways in Eq. A.23 and Eq. A.28 to **re-estimate** the transition and observation probabilities $(A,B)$ from an observation sequence $O$, assuming that we already have a previous estimate of $A$ and $B$.

\

These re-estimations form the core of the iterative forward-backward algorithm. The forward-backward algorithm (Fig. A.14) starts with some initial estimate of the HMM parameters $\lambda = (A,B)$. We then iteratively run two steps. Like other cases of the EM (expectation-maximization) algorithm, the forward-backward algorithm has two steps: the **_expectation_** step (E-step), and the **_maximization_** step (M-step).

\

**In the E-step**, we compute the expected state occupancy count $\gamma$ and the expected transition count $\xi$ from the earlier $A$ and $B$ probabilities. **In the M-step**, we use $\gamma$ and $\xi$ to recompute new $A$ and $B$ probabilities.

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images_paper/ss19.png)

\

Although, in principle the forward-backward algorithm can do completely unsupervised learning of the $A$ and $B$ parameters, in practice the initial conditions are very important. For this reason the algorithm is often given extra information.

For example, for HMM-based speech recognition, the HMM structure is often set by hand, and only the emission ($B$) and (non-zero) $A$ transition probabilities are trained from a set of observation sequences $O$.

\
\
\

#### Implementing the Baum-Welch algorithm using the 'HMM' package:

\

Refer to the ice cream example,

```{r}
library(HMM)
# Initialise HMM
hmm = initHMM(
  States=c("C","H"),
  Symbols=c(1,2,3)
)
print(hmm)
```

```{r}
# Sequence of observations
observations = c(3,1,3)  # o_1, o_2, o_3

# Estimating A,B matrices
bw = baumWelch(hmm, observations, 100)
print(bw$hmm)
```

The estimated $A$ and $B$ matrices seem pretty off from the actual A and B given in the ice cream example, and this is most likely due to the lack of data in the given observation sequence $O$.

\
\
\
\

### 6. Summary

\

A summary for the hidden Markov model for probabilistic sequence classification is as follows.

\

- Hidden Markov models (HMMs) are a way of relating a sequence of **observations** to a sequence of **hidden states** (or classes) that explain the observations.

- For HMM, we're concerned with **three fundamental problems**: Likelihood, Decoding, and Learning.

- The computation of **likelihood** of a particular observation sequence is done by the **forward** algorithm.

- The process of discovering the sequence of hidden states, given the sequence of observations, is known as **decoding** (or inference). The **Viterbi** algorithm is commonly used for decoding.

- The parameters of an HMM are the transition probability matrix, $A$, and the observation likelihood matrix, $B$. Both can be **learned** (or trained) with the **Baum-Welch** (or forward-backward) algorithm.

\
\
\
\



