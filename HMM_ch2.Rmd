---
title: "HMM"
subtitle: "Ch.2 Hidden Markov models: definition and properties"
author: "Jaeyong Lee"
output:
  html_notebook:
    toc: yes
    code_folding: "none"
---

<style type="text/css">
h1.title {
  font-size: 30px;
  text-align: center;
}
h3.subtitle {
  font-size: 20px;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
    font-size: 18px;
  text-align: right;
}
body{
   font-size: 17px;  # body is for normal text
}
td{
   font-size: 12px;  # td is for table data
}
</style>

\
\

### A simple hidden Markov model

\

The Earthquake data's ACF from the previous chapter shows a serial dependence. 

```{r}
library(fpp2)
raw <- c(13,14,8,10,16,26,32,27,18,32,36,24,22,23,22,18,25,21,21,14,
          8,11,14,23,18,17,19,20,22,19,13,26,13,14,22,24,21,22,26,21,
          23,24,27,41,31,27,35,26,28,36,39,21,17,22,17,19,15,34,10,15,
          22,18,15,20,15,22,19,16,30,27,29,23,20,16,21,21,25,16,18,15,
          18,14,10,15,8,15,6,11,8,7,18,16,13,12,13,20,15,16,12,18,
          15,16,13,15,16,11,11)
data <- ts(raw, start=1900, frequency=1)
ggAcf(data, lag=15)
```

\

An independent mixture model will not do for the earthquake series because – by definition – it does not allow for the serial dependence in the observations.

One way of allowing for serial dependence in the observations is to relax the assumption that the parameter process is serially independent. A simple and mathematically convenient way to do so is to assume that it is a Markov chain. The resulting model for the observations is called a Poisson–hidden Markov model, a simple example of the class of hidden markov models (HMMs).

\
\
\
\

### The basics

\

#### Definition

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images/ss8.png)

\
\

If the Markov chain {Ct} has m states, we call {Xt} an m-state HMM.

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images/ss9.png)

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images/ss10.png)

\
\
\

#### Marginal distributions

\

Univariate distributions:

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images/ss11.png)

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images/ss12.png)

\
\

Bivariate distributions:

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images/ss13.png)

\
\
\

#### Moments

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images/ss14.png)

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images/ss15.png)

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images/ss16.png)

\
\
\
\

### The likelihood

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images/ss17.png)

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images/ss18.png)

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images/ss23.png)

\
\

**The likelihood in general**

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images/ss19.png)

\
\

A very simple but crucial consequence of the matrix expression for the likelihood is the ‘forward algorithm’ for recursive computation of the likelihood. Such recursive computation plays a key role, not only in likelihood evaluation and hence parameter estimation, but also in forecasting, decoding and model checking. The recursive nature of likelihood evaluation via either (2.12) or (2.13) is computationally much more efficient than brute-force summation over all possible state sequences.

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images/ss20.png)

\
\
\

#### HMMs are not Markov processes

\

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images/ss21.png)

![](/Users/jaeyonglee/Documents/College/RStudio/HMM/HMM/images/ss22.png)

\
\
\

Additional topics: 

- The likelihood when data are missing
- The likelihood when observations are interval-censored

\
\
\
\



















